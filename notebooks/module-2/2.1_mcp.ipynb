{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "717edf63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db64c0fd-9355-49e4-8290-2f2ae08eeef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import asyncio\n",
    "\n",
    "# Fix for Windows issues in Jupyter notebooks\n",
    "if sys.platform == \"win32\":\n",
    "    # 1. Use ProactorEventLoop for subprocess support\n",
    "    if not isinstance(asyncio.get_event_loop_policy(), asyncio.WindowsProactorEventLoopPolicy):\n",
    "        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())\n",
    "    \n",
    "    # 2. Redirect stderr to avoid fileno() error when launching MCP servers\n",
    "    if \"ipykernel\" in sys.modules:\n",
    "        sys.stderr = sys.__stderr__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d701224",
   "metadata": {},
   "source": [
    "## Local MCP server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f11678d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"local_server\": {\n",
    "                \"transport\": \"stdio\",\n",
    "                \"command\": \"python\",\n",
    "                \"args\": [\"resources/2.1_mcp_server.py\"],\n",
    "            }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "184db1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tools\n",
    "tools = await client.get_tools()\n",
    "\n",
    "# get resources\n",
    "resources = await client.get_resources(\"local_server\")\n",
    "\n",
    "# get prompts\n",
    "prompt = await client.get_prompt(\"local_server\", \"prompt\")\n",
    "prompt = prompt[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d548fad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-5-nano\",\n",
    "    tools=tools,\n",
    "    system_prompt=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5256ae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "response = await agent.ainvoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Tell me about the langchain-mcp-adapters library\")]},\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3efb5bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='Tell me about the langchain-mcp-adapters library', additional_kwargs={}, response_metadata={}, id='2d780a3d-b958-4410-b8a1-46c5b2cca8e9'),\n",
      "              AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 156, 'prompt_tokens': 271, 'total_tokens': 427, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 128, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CyNXDWieKGPDO42iD821LmtEtnJEv', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019bc32e-c972-7353-be6d-5813b5e80aea-0', tool_calls=[{'name': 'search_web', 'args': {'query': 'langchain-mcp-adapters'}, 'id': 'call_zTovXI2OaGaIATHFHbr3Kg2B', 'type': 'tool_call'}], usage_metadata={'input_tokens': 271, 'output_tokens': 156, 'total_tokens': 427, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 128}}),\n",
      "              ToolMessage(content=[{'type': 'text', 'text': '{\\n  \"query\": \"langchain-mcp-adapters\",\\n  \"follow_up_questions\": null,\\n  \"answer\": null,\\n  \"images\": [],\\n  \"results\": [\\n    {\\n      \"url\": \"https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph\",\\n      \"title\": \"MCP Adapters for LangChain and LangGraph\",\\n      \"content\": \"# LangChain Changelog. Sign up for our newsletter to stay up to date. # MCP Adapters for LangChain and LangGraph. The **LangChain MCP Adapters** is a package that makes it easy to use **Anthropic Model Context Protocol (MCP) tools** with LangChain & LangGraph. * Converts **MCP tools** into **LangChain- & LangGraph-compatible tools**. * Enables interaction with tools across multiple **MCP servers**. * Seamlessly integrates the **hundreds of tool servers** already published into LangGraph Agents. ### Why use MCP Adapters:. This adapter makes it simple to connect LangChain and LangGraph with the growing ecosystem of MCP tool servers. Instead of manually adapting each tool, you can now integrate them seamlessly. It also allows agents to pull from multiple MCP servers at once, making it easier to combine different tools for more powerful applications. MCP is gaining serious traction, and this adapter helps LangGraph agents take full advantage. ##### Subscribe to updates.\",\\n      \"score\": 0.9999956,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://pypi.org/project/langchain-mcp-tools/\",\\n      \"title\": \"langchain-mcp-tools\",\\n      \"content\": \"LangChain\\'s official LangChain MCP Adapters library, which supports comprehensive integration with LangChain, has been released. You may want to consider using\",\\n      \"score\": 0.9999918,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://www.npmjs.com/package/@langchain/mcp-adapters\",\\n      \"title\": \"langchain/mcp-adapters\",\\n      \"content\": \"The library allows you to connect to one or more MCP servers and load tools from them, without needing to manage your own MCP client instances. // Whether to throw on errors if a tool fails to load (optional, default: true). // Whether to prefix tool names with the server name (optional, default: false). // Whether to throw errors if a tool fails to load (optional, default: true). When calling tools from the `camera` MCP server, the following `outputHandling` config will be used:. Similarly, when calling tools on the `microphone` MCP server, the following `outputHandling` config will be used:. You can include a `defaultToolTimeout` field in the server config to set the timeout for all tools for that server, or globally for the entire client by setting it in the top-level config. For secure MCP servers that require OAuth 2.0 authentication, you can use the `authProvider` option instead of manually managing headers. const tools = await client.getTools(); // Only tools from \\\\\"working-server\\\\\".\",\\n      \"score\": 0.9999831,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://reference.langchain.com/python/langchain_mcp_adapters/\",\\n      \"title\": \"langchain-mcp-adapters\",\\n      \"content\": \"# `langchain-mcp-adapters`¶. Client for connecting to multiple MCP servers and loading LC tools/resources. This module provides the `MultiServerMCPClient` class for managing connections to multiple MCP servers and loading tools, prompts, and resources from them. Loads LangChain-compatible tools, prompts and resources from MCP servers. | \\\\\\\\_\\\\\\\\_init\\\\\\\\_\\\\\\\\_ (`langchain_mcp_adapters.client.MultiServerMCPClient.__init__`)\\\\\">\\\\\\\\_\\\\\\\\_init\\\\\\\\_\\\\\\\\_ | Initialize a `MultiServerMCPClient` with MCP servers connections. | session  `async`  (`langchain_mcp_adapters.client.MultiServerMCPClient.session`)\\\\\">session | Connect to an MCP server and initialize a session. | get\\\\\\\\_tools  `async`  (`langchain_mcp_adapters.client.MultiServerMCPClient.get_tools`)\\\\\">get\\\\\\\\_tools | Get a list of all tools from all connected servers. | get\\\\\\\\_resources  `async`  (`langchain_mcp_adapters.client.MultiServerMCPClient.get_resources`)\\\\\">get\\\\\\\\_resources | Get resources from MCP server(s). **TYPE:** `dict[str,`  Connection  `module-attribute`  (`langchain_mcp_adapters.sessions.Connection`)\\\\\">Connection] | None   **DEFAULT:** `None` |. ### load\\\\\\\\_mcp\\\\\\\\_tools `async` ¶. (langchain_mcp_adapters.callbacks.Callbacks)\\\\\">Callbacks | None = None,  tool_interceptors: list[            ToolCallInterceptor (langchain_mcp_adapters.interceptors.ToolCallInterceptor)\\\\\">ToolCallInterceptor] | None = None,  server_name: str | None = None,  tool_name_prefix: bool = False, ) -> list[            BaseTool (langchain_core.tools.BaseTool)\\\\\">BaseTool]. **TYPE:**  Connection  `module-attribute`  (`langchain_mcp_adapters.sessions.Connection`)\\\\\">Connection | None   **DEFAULT:** `None` |. ### load\\\\\\\\_mcp\\\\\\\\_resources `async` ¶. | \\\\\\\\_\\\\\\\\_call\\\\\\\\_\\\\\\\\_  `async`  (`langchain_mcp_adapters.interceptors.ToolCallInterceptor.__call__`)\\\\\">\\\\\\\\_\\\\\\\\_call\\\\\\\\_\\\\\\\\_ | Intercept tool execution with control over handler invocation.\",\\n      \"score\": 0.9999784,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://medium.com/@deepakkamboj/the-complete-guide-to-langchain-mcp-adapters-bridging-langchain-and-model-context-protocol-3f5507cbd3ca\",\\n      \"title\": \"The Complete Guide to langchain-mcp-adapters\",\\n      \"content\": \"This open-source package eliminates the complexity of manual tool integration by providing seamless conversion between MCP tools and LangChain-compatible tools, enabling developers to tap into hundreds of existing MCP servers without writing custom adapters. from langchain_mcp_adapters.client import MultiServerMCPClientfrom langchain.agents import create_agentclient = MultiServerMCPClient({ \\\\\"math\\\\\": { \\\\\"transport\\\\\": \\\\\"stdio\\\\\", \\\\\"command\\\\\": \\\\\"python\\\\\", \\\\\"args\\\\\": [\\\\\"/path/to/math_server.py\\\\\"] }, \\\\\"weather\\\\\": { \\\\\"transport\\\\\": \\\\\"streamable_http\\\\\", \\\\\"url\\\\\": \\\\\"http://localhost:8000/mcp\\\\\" }})# Load all tools from all serverstools = await client.get_tools()# Create an agent with these toolsagent = create_agent(\\\\\"claude-sonnet-4-5-20250929\\\\\", tools)# Use the agentresponse = await agent.ainvoke({ \\\\\"messages\\\\\": [{\\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"what\\'s (3 + 5) x 12?\\\\\"}]}). from langchain_mcp_adapters.client import MultiServerMCPClientfrom langgraph.graph import StateGraph, MessagesState, STARTfrom langgraph.prebuilt import ToolNode, tools_conditionfrom langchain.chat_models import init_chat_modelmodel = init_chat_model(\\\\\"openai:gpt-4.1\\\\\")client = MultiServerMCPClient({ \\\\\"math\\\\\": { \\\\\"command\\\\\": \\\\\"python\\\\\", \\\\\"args\\\\\": [\\\\\"./examples/math_server.py\\\\\"], \\\\\"transport\\\\\": \\\\\"stdio\\\\\" }, \\\\\"weather\\\\\": { \\\\\"url\\\\\": \\\\\"http://localhost:8000/mcp\\\\\", \\\\\"transport\\\\\": \\\\\"streamable_http\\\\\" }})tools = await client.get_tools()def call_model(state: MessagesState): response = model.bind_tools(tools).invoke(state[\\\\\"messages\\\\\"]) return {\\\\\"messages\\\\\": response}builder = StateGraph(MessagesState)builder.add_node(call_model)builder.add_node(ToolNode(tools))builder.add_edge(START, \\\\\"call_model\\\\\")builder.add_conditional_edges(\\\\\"call_model\\\\\", tools_condition)builder.add_edge(\\\\\"tools\\\\\", \\\\\"call_model\\\\\").\",\\n      \"score\": 0.9999577,\\n      \"raw_content\": null\\n    }\\n  ],\\n  \"response_time\": 1.88,\\n  \"request_id\": \"4ebe83d3-e114-4eb4-a43a-e1fbbd50e300\"\\n}', 'id': 'lc_af541de7-289d-445f-9cea-f24f469a0515'}], name='search_web', id='46ff62b5-3f69-4ae6-b6e7-1dcc4313c99f', tool_call_id='call_zTovXI2OaGaIATHFHbr3Kg2B', artifact={'structured_content': {'result': {'query': 'langchain-mcp-adapters', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph', 'title': 'MCP Adapters for LangChain and LangGraph', 'content': '# LangChain Changelog. Sign up for our newsletter to stay up to date. # MCP Adapters for LangChain and LangGraph. The **LangChain MCP Adapters** is a package that makes it easy to use **Anthropic Model Context Protocol (MCP) tools** with LangChain & LangGraph. * Converts **MCP tools** into **LangChain- & LangGraph-compatible tools**. * Enables interaction with tools across multiple **MCP servers**. * Seamlessly integrates the **hundreds of tool servers** already published into LangGraph Agents. ### Why use MCP Adapters:. This adapter makes it simple to connect LangChain and LangGraph with the growing ecosystem of MCP tool servers. Instead of manually adapting each tool, you can now integrate them seamlessly. It also allows agents to pull from multiple MCP servers at once, making it easier to combine different tools for more powerful applications. MCP is gaining serious traction, and this adapter helps LangGraph agents take full advantage. ##### Subscribe to updates.', 'score': 0.9999956, 'raw_content': None}, {'url': 'https://pypi.org/project/langchain-mcp-tools/', 'title': 'langchain-mcp-tools', 'content': \"LangChain's official LangChain MCP Adapters library, which supports comprehensive integration with LangChain, has been released. You may want to consider using\", 'score': 0.9999918, 'raw_content': None}, {'url': 'https://www.npmjs.com/package/@langchain/mcp-adapters', 'title': 'langchain/mcp-adapters', 'content': 'The library allows you to connect to one or more MCP servers and load tools from them, without needing to manage your own MCP client instances. // Whether to throw on errors if a tool fails to load (optional, default: true). // Whether to prefix tool names with the server name (optional, default: false). // Whether to throw errors if a tool fails to load (optional, default: true). When calling tools from the `camera` MCP server, the following `outputHandling` config will be used:. Similarly, when calling tools on the `microphone` MCP server, the following `outputHandling` config will be used:. You can include a `defaultToolTimeout` field in the server config to set the timeout for all tools for that server, or globally for the entire client by setting it in the top-level config. For secure MCP servers that require OAuth 2.0 authentication, you can use the `authProvider` option instead of manually managing headers. const tools = await client.getTools(); // Only tools from \"working-server\".', 'score': 0.9999831, 'raw_content': None}, {'url': 'https://reference.langchain.com/python/langchain_mcp_adapters/', 'title': 'langchain-mcp-adapters', 'content': '# `langchain-mcp-adapters`¶. Client for connecting to multiple MCP servers and loading LC tools/resources. This module provides the `MultiServerMCPClient` class for managing connections to multiple MCP servers and loading tools, prompts, and resources from them. Loads LangChain-compatible tools, prompts and resources from MCP servers. | \\\\_\\\\_init\\\\_\\\\_ (`langchain_mcp_adapters.client.MultiServerMCPClient.__init__`)\">\\\\_\\\\_init\\\\_\\\\_ | Initialize a `MultiServerMCPClient` with MCP servers connections. | session  `async`  (`langchain_mcp_adapters.client.MultiServerMCPClient.session`)\">session | Connect to an MCP server and initialize a session. | get\\\\_tools  `async`  (`langchain_mcp_adapters.client.MultiServerMCPClient.get_tools`)\">get\\\\_tools | Get a list of all tools from all connected servers. | get\\\\_resources  `async`  (`langchain_mcp_adapters.client.MultiServerMCPClient.get_resources`)\">get\\\\_resources | Get resources from MCP server(s). **TYPE:** `dict[str,`  Connection  `module-attribute`  (`langchain_mcp_adapters.sessions.Connection`)\">Connection] | None   **DEFAULT:** `None` |. ### load\\\\_mcp\\\\_tools `async` ¶. (langchain_mcp_adapters.callbacks.Callbacks)\">Callbacks | None = None,  tool_interceptors: list[            ToolCallInterceptor (langchain_mcp_adapters.interceptors.ToolCallInterceptor)\">ToolCallInterceptor] | None = None,  server_name: str | None = None,  tool_name_prefix: bool = False, ) -> list[            BaseTool (langchain_core.tools.BaseTool)\">BaseTool]. **TYPE:**  Connection  `module-attribute`  (`langchain_mcp_adapters.sessions.Connection`)\">Connection | None   **DEFAULT:** `None` |. ### load\\\\_mcp\\\\_resources `async` ¶. | \\\\_\\\\_call\\\\_\\\\_  `async`  (`langchain_mcp_adapters.interceptors.ToolCallInterceptor.__call__`)\">\\\\_\\\\_call\\\\_\\\\_ | Intercept tool execution with control over handler invocation.', 'score': 0.9999784, 'raw_content': None}, {'url': 'https://medium.com/@deepakkamboj/the-complete-guide-to-langchain-mcp-adapters-bridging-langchain-and-model-context-protocol-3f5507cbd3ca', 'title': 'The Complete Guide to langchain-mcp-adapters', 'content': 'This open-source package eliminates the complexity of manual tool integration by providing seamless conversion between MCP tools and LangChain-compatible tools, enabling developers to tap into hundreds of existing MCP servers without writing custom adapters. from langchain_mcp_adapters.client import MultiServerMCPClientfrom langchain.agents import create_agentclient = MultiServerMCPClient({ \"math\": { \"transport\": \"stdio\", \"command\": \"python\", \"args\": [\"/path/to/math_server.py\"] }, \"weather\": { \"transport\": \"streamable_http\", \"url\": \"http://localhost:8000/mcp\" }})# Load all tools from all serverstools = await client.get_tools()# Create an agent with these toolsagent = create_agent(\"claude-sonnet-4-5-20250929\", tools)# Use the agentresponse = await agent.ainvoke({ \"messages\": [{\"role\": \"user\", \"content\": \"what\\'s (3 + 5) x 12?\"}]}). from langchain_mcp_adapters.client import MultiServerMCPClientfrom langgraph.graph import StateGraph, MessagesState, STARTfrom langgraph.prebuilt import ToolNode, tools_conditionfrom langchain.chat_models import init_chat_modelmodel = init_chat_model(\"openai:gpt-4.1\")client = MultiServerMCPClient({ \"math\": { \"command\": \"python\", \"args\": [\"./examples/math_server.py\"], \"transport\": \"stdio\" }, \"weather\": { \"url\": \"http://localhost:8000/mcp\", \"transport\": \"streamable_http\" }})tools = await client.get_tools()def call_model(state: MessagesState): response = model.bind_tools(tools).invoke(state[\"messages\"]) return {\"messages\": response}builder = StateGraph(MessagesState)builder.add_node(call_model)builder.add_node(ToolNode(tools))builder.add_edge(START, \"call_model\")builder.add_conditional_edges(\"call_model\", tools_condition)builder.add_edge(\"tools\", \"call_model\").', 'score': 0.9999577, 'raw_content': None}], 'response_time': 1.88, 'request_id': '4ebe83d3-e114-4eb4-a43a-e1fbbd50e300'}}}),\n",
      "              AIMessage(content='Here’s a concise overview of the langchain-mcp-adapters library and what it’s for.\\n\\nWhat it is\\n- LangChain MCP Adapters is a library that makes it easy to connect LangChain and LangGraph with MCP (Model Context Protocol) tool servers.\\n- It converts MCP tools into LangChain- and LangGraph-compatible tools, enabling interaction with tools across multiple MCP servers.\\n- It helps agents pull tools from many MCP servers at once and integrates them into LangGraph agents.\\n\\nKey features\\n- Multi-server adoption\\n  - Connect to multiple MCP servers with a single client and load tools, prompts, and resources from all of them.\\n- Easy conversion\\n  - Automatically converts MCP tools into LangChain-compatible tools so you can use them with LangChain and LangGraph agents.\\n- Centralized management\\n  - Manage tool loading, resource loading, and sessions across servers from one place.\\n- Language bindings\\n  - Python and JavaScript/Node support\\n    - Python: langchain-mcp-tools (the official Python MCP adapters library)\\n    - Node: @langchain/mcp-adapters (NPM package)\\n- Configuration and security options\\n  - Options to throw on load errors, prefix tool names with the server name, and set a default tool timeout.\\n  - authProvider support for OAuth 2.0 on secure MCP servers.\\n- API surface (high level)\\n  - MultiServerMCPClient: main entry point for connecting to MCP servers\\n    - session(): establish a session with the MCP servers\\n    - get_tools(): fetch a list of all tools from connected servers\\n    - get_resources(): fetch resources from MCP servers\\n    - load_mcp_tools(...): load MCP tools as LangChain tools (supports server_name, tool_name_prefix, etc.)\\n    - load_mcp_resources(...): load MCP resources\\n  - ToolCallInterceptor (optional): intercepts tool calls for custom handling/observability\\n\\nWhy you’d use it\\n- If you already have or want to leverage the large ecosystem of MCP tool servers, this adapters package lets you use them without writing custom adapters for each server.\\n- It’s especially useful when building LangGraph agents that need to orchestrate tools from multiple MCP servers or when you want to mix dozens/hundreds of MCP-based tools with LangChain prompts and workflows.\\n\\nWhat to read for details\\n- Changelog/announcements: MCP Adapters for LangChain and LangGraph (high-level overview, motivation, and capabilities)\\n- Python docs: reference.langchain.com/python/langchain_mcp_adapters/\\n  - Covers MultiServerMCPClient, session, get_tools, get_resources, load_mcp_tools, load_mcp_resources, and related concepts\\n- PyPI package: langchain-mcp-tools (official Python MCP adapters library)\\n- NPM package: @langchain/mcp-adapters (Node.js / JavaScript version)\\n- Practical guides and overviews\\n  - The Complete Guide to langchain-mcp-adapters (Medium article) with examples and recipes\\n  - Tongue-in-cheek summaries and example snippets in various blog posts\\n\\nShort example ideas (conceptual)\\n- Python (simplified)\\n  - client = MultiServerMCPClient({\"math\": {\"transport\": \"stdio\", \"command\": \"python\", \"args\": [\"./math_server.py\"]}, \"weather\": {\"transport\": \"streamable_http\", \"url\": \"http://localhost:8000/mcp\"}})\\n  - tools = await client.get_tools()\\n  - agent = create_agent(..., tools)\\n  - response = await agent.ainvoke({\"messages\": [{\"role\": \"user\", \"content\": \"What\\'s (3+5) x 12?\"}]})\\n\\n- Node.js (conceptual)\\n  - const client = new MultiServerMCPClient({ \"math\": { transport: \"stdio\", command: \"node\", args: [\"./math_server.js\"] }, \"weather\": { transport: \"streamable_http\", url: \"http://localhost:8000/mcp\" } });\\n  - const tools = await client.getTools();\\n  - const agent = create_agent(name, tools);\\n  - const result = await agent.invoke({ messages: [{ role: \"user\", content: \"What\\'s the weather like in Paris?\" }] });\\n\\nInstallation quick start\\n- Python\\n  - pip install langchain-mcp-tools\\n- Node\\n  - npm i @langchain/mcp-adapters\\n\\nWould you like:\\n- a minimal, copy-paste code example in Python to get started with a simple MCP server setup?\\n- a quick Node.js example to illustrate loading MCP tools into a LangGraph agent?\\n- pointers to the most up-to-date documentation or a link to a specific setup (multi-server config, OAuth, tool_name_prefix, etc.)?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 3034, 'prompt_tokens': 2075, 'total_tokens': 5109, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 2048, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CyNXJ9CVgP9RcfYkWC5IgNawcydfL', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019bc32e-e365-7ca2-a18b-9ade6369a762-0', usage_metadata={'input_tokens': 2075, 'output_tokens': 3034, 'total_tokens': 5109, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 2048}})]}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847409a3",
   "metadata": {},
   "source": [
    "## Online MCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b2895fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"time\": {\n",
    "            \"transport\": \"stdio\",\n",
    "            \"command\": \"uvx\",\n",
    "            \"args\": [\n",
    "                \"mcp-server-time\",\n",
    "                \"--local-timezone=America/New_York\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "tools = await client.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e264dd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    model=\"gpt-5-nano\",\n",
    "    tools=tools,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4725cee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='What time is it?', additional_kwargs={}, response_metadata={}, id='99aae70e-8f7c-48ad-b95f-d8486b798940'),\n",
      "              AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 155, 'prompt_tokens': 296, 'total_tokens': 451, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 128, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CyNjrXMIjntADBGDfBNL3ZcmC2ILG', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019bc33a-c276-7673-9875-4e3e3ff63a49-0', tool_calls=[{'name': 'get_current_time', 'args': {'timezone': 'America/New_York'}, 'id': 'call_2Huty6TaxSQYXENlYYul6OuM', 'type': 'tool_call'}], usage_metadata={'input_tokens': 296, 'output_tokens': 155, 'total_tokens': 451, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 128}}),\n",
      "              ToolMessage(content=[{'type': 'text', 'text': '{\\n  \"timezone\": \"America/New_York\",\\n  \"datetime\": \"2026-01-15T14:56:11-05:00\",\\n  \"day_of_week\": \"Thursday\",\\n  \"is_dst\": false\\n}', 'id': 'lc_278cc803-ec13-4fd8-b63b-87cd68c20990'}], name='get_current_time', id='ff5725f7-77ec-4470-97d1-5821d040fac1', tool_call_id='call_2Huty6TaxSQYXENlYYul6OuM'),\n",
      "              AIMessage(content='In New York (Eastern Standard Time), it’s Thursday, January 15, 2026 at 2:56 PM (UTC-5). DST is not in effect right now.\\n\\nWant me to convert this to another time zone?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 377, 'prompt_tokens': 379, 'total_tokens': 756, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 320, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CyNjv7jVYCqqPvKdVfEXEA0JcNkko', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019bc33a-d5e7-7a22-8371-0de4dd14f2fa-0', usage_metadata={'input_tokens': 379, 'output_tokens': 377, 'total_tokens': 756, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 320}})]}\n"
     ]
    }
   ],
   "source": [
    "question = HumanMessage(content=\"What time is it?\")\n",
    "\n",
    "response = await agent.ainvoke(\n",
    "    {\"messages\": [question]}\n",
    ")\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bc5152",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc-foundations",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
